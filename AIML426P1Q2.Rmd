---
title: "AIML426 Project 1 Q2"
author:  Chad Kakau
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
library(reticulate)
```

# AIML426 Project 1 Q2  

## Genetic Algorithm for Feature Selection  

Genetic algorithm can be used for feature selection, given $N$ features, each feature selection result can be represented as a $N$-dimensional binary list $X=(x_1, ..., x_n)$, where $x_i=1$ means the feature $i$ is selected and $x_i=0$ otherwise.  

### Problem description  

Take a dataset with $N$ features and determine the optimal selection of features for fitting a good predictive model.  The task is to build a Genetic Algorithm to perform feature selection, by selecting the fewest features that provide good classification.  The task requires two approaches:  

  - __Filter function__:  where the fitness evaluation is conducted without a classifier function  
  - __Wrapper function__:  where the fitness evaluation is conducted by assessing the classification accuracy of the features when used with a classification model.  
  
### evaluation function:  FilterGA  

The Filter function compares average mutual gain information per feature.  The evaluation function computes the information gain for the subset of features for an individual (i.e. features where $x_i = 1$) and then averages that information gain across the selected features, to give an average information gain per feature.  The sklearn.feature_selection.mutual_info_classif function allows for identification of discrete variables and can handle continuous variables.  

$$
\begin{aligned}
\text{Fit}_{avg IG} & = \frac{I(Y;X)}{m}, \text{ with } m = \text{no. of features in X}\\
 \\
 I(Y;X)& = H(X)+H(Y)-H(X,Y)\\
\text{where } H(Y) & = -\sum_y p(y)*log_2 p(y),\\
H(X) & = -\sum_y p(x)*log_2 p(x),\\
 H(X,Y) & = -\sum_{x,y} p(x, y)*log_2 p(x, y),\\
\end{aligned}
$$

Average information gain per feature has a maximising objective.  Number of features has a minimising objective. 

I chose the mutual gain approach because I thought it would be quicker to calculate and was so confused when the results took longer than the Wrapper method.  I figured out that because I was using the sklearn.feature_selection.mutual_info_classif function and this running a full modedlling process for each individual.  In the end I calculated the entropies for each class $(H(Y))$, each feature $(H(X_i))$, and the joint entropies for each feature and class $(H(X_i, Y))$and stored those values at read-in.  Using this method, signficiantly reduced the time for calculating mutual gain for each individual.

### evaluation function:  Wrapper

The Wrapper function uses the sklearn.neighbors.KNeighborsClassifier function to fit a model to the reduced feature set and determines accuracy of that fitted model against a test set.  The K-Neighbors Classifier is set to $k = 2$.

I chose this method because I already knew how many classes there were in the categorical dataset.  Even without knowing, I could have easily established the number of classes by using something like the pandas.Series().values_count() function to determine counts of each class and the number of classes.  That means I could modify the code to accept multi-class objectives.

### Parameter settings  

I 

```{python importAndFuncCreation, include=FALSE}
# import function
import feat_select as fs


# function for running experiment N-times
def multi_run(func, data = None, feat_names = None, params = None, n=1):
    from datetime import datetime as dt
    # accept only singular parameters - i.e. feed one function only
    best_indiv = []
    run_stats = []
    time_delta = []
    population = []

    for r in range(n): 
        start = dt.now()
        if feat_names:
            if params:
                pop, stat, hof = func(data, feat_names, params)
                
            else:
                pop, stat, hof = func(data, feat_names)
                
        else: 
            pop, stat, hof = func(data, params)

        
        end = dt.now()
        time = end - start

        run_stats.append(stat)
        best_indiv.append(hof)
        time_delta.append(time)
        population.append(pop)
                
    return run_stats, best_indiv, time_delta, population

# function for running through the different datasets or different functions
def changing_run(paths = None, feat_names = None, func = None, params = None, n = 1):

    diffs = [[], [], [], []] 

    if len(paths) > 1:
        # iterate paths, keep all other parameters the same, function is required, params are optional
        for p, dat in enumerate(paths):
            if len(feat_names) > 1:
                # include feat_names input
                if params:
                    run, best, time, population = multi_run(func, 
                                           data= dat, 
                                           feat_names= feat_names[p], 
                                           params = params, 
                                           n = n)
                else:
                    run, best, time, population = multi_run(func, 
                                           data= dat, 
                                           feat_names= feat_names[p], 
                                           n = n)
            
            else:
                
                if params:
                    run, best, time, population = multi_run(func, 
                                           data= dat, 
                                           params = params, 
                                           n = n)
                else:
                    run, best, time, population = multi_run(func, 
                                           data= dat, 
                                           n = n)
            diffs[0].append(run)
            diffs[1].append(best)
            diffs[2].append(time)
            diffs[3].append(population)

    return diffs
```

```{python readData}
import pandas as pd
df = pd.read_csv('wbcd.data', header = None, names = None)
X = df.iloc[:, :(len(df.columns)-1)]
y = df.iloc[:, len(df.columns)-1]
```


```{python testingForDiscretFunc}
# clear that variable
# del X_disc

# create an empty df with appropriate dim
X_disc = pd.DataFrame(columns = X.columns, index = X.index)

# cut each column into 10 bins
for i, _ in enumerate(X.columns): 
  X_disc.iloc[:, i] = pd.cut(df.iloc[:, i], bins = 10,)
  # retain just the left boundary of the interval for each value in the column
  for r, _ in enumerate(X_disc.iloc[:, i]):
    X_disc.loc[r, i] = X_disc.iloc[r, i].left
    
```


```{python buildFunction}

def discretize_data(X):
  import pandas as pd
  
  X_disc = pd.DataFrame(columns = X.columns, index = X.index)
  
  # cut each column into 10 mins
  for column, _ in enumerate(X.columns): 
    X_disc.iloc[:, column] = pd.cut(X.iloc[:, column], bins = 10,)
    
    # keep the value for the left interval
    for row, _ in enumerate(X_disc.iloc[:, column]):
      X_disc.iloc[row, column] = X_disc.iloc[row, column].left
      
  return X_disc

```

```{python getThebest}
# function to retun the best individual in a population based on fitness
def best_in_population(pop):
  from operator import attrgetter
  
  # retrieve from the population, the individual with the best fitness
  best = max(pop, key = attrgetter('fitness'))
  
  return best


# function to extract X based on best individual's feature selection
def selected_features(best, X):
  import pandas as pd
  
  selected = []
  
  # create a set of selected features based on the chromosomes of the best individual
  for f, feature in enumerate(best):
    
    # retain the selected features
    if feature == 1:
      selected.append(f)
    
  # generate the reduced X with just selected features
  reduced_X = X.iloc[:, selected]
  
  return reduced_X


# function to determine training accuracy of a dataset using DecisionTreeClassifier
def accuracy_of_selected_features(X, y):
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.model_selection import train_test_split
  
  # split data for training and testing
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4)
  
  # run the classifier
  clf = DecisionTreeClassifier()
  clf = clf.fit(X_train, y_train)
  
  accuracy = clf.score(X_test, y_test)
  
  return accuracy
```



```{python functionsForEntropy}
# create a function to calculate probabilities of discretised vectors
def joint_probabilities(X_i, y):
  import pandas as pd
  import numpy as np
  
  # establish classes
  classes_of_y = y.unique()
  levels_of_X_i = X_i.unique()
  
  # establish output object
  probs = pd.DataFrame(index = levels_of_X_i, columns = classes_of_y, )
  # establish probabilities for each category of x
  # counts
  for clas in classes_of_y:
    count = pd.Series(X_i[y==clas]).value_counts()
    prob = count/len(X_i)
    probs[clas] = prob
    
  return probs
    
# create a function to calculate entropy
def entropy(probabilities):
  import pandas as pd
  import numpy as np
  
  entropy = 0
  
  # allow for single columns
  if isinstance(probabilities, pd.Series):
    ent = pd.DataFrame(columns = probabilities.index)
    ent = - np.sum(probabilities* np.log2(probabilities))
    
    entropy = ent
  
  else:
    # compute Shannon's entropy for 
    for prob in probabilities.columns:
    
      ent = - np.sum(probabilities.loc[:, prob]* np.log2(probabilities.loc[:, prob]))
      
      entropy += ent
    
  entropy = -entropy  
  return entropy

#create a reference matrix for conditional probabilities 

# create function to calculate information gain I(X;Y) = H(X) +H(Y) - H(X, Y)
def mutual_information(features, joint = None, indiv = None, clas = None):
  import numpy as np
  
  # features is an array of indexes for features
  H_X = 0
  H_XY = 0
  H_Y = clas
  for feature in features:
    H_X += indiv[feature]
    H_XY += joint[feature]
    
  MI = H_X +H_Y - H_XY
  
  return MI

```


```{python testingEntropies}
joint_entropies = [entropy(joint_probabilities(X_disc.loc[:, i], y)) for i in X_disc.columns]
individual_entropies = [entropy(pd.Series(X_disc.loc[:, i]).value_counts()/len(y)) for i in X_disc.columns]
class_entropy = entropy(pd.Series(y).value_counts()/len(y))

mutual_information([1,3,7,21], joint = joint_entropies, indiv = individual_entropies, clas = class_entropy)

```


```{python wrapperExperiments, include = FALSE}
# test the wrapper function
WrapperGA = changing_run(func = fs.main,
                         params = "FEAT_SEL = 'Wrapper'",
                         paths = ['wbcd.data', 'sonar.data'],
                         feat_names = ['wbcd.names', 'sonar.names'],
                         n = 5)  
```

```{python filterExperiments, include = FALSE}
# test the filter function
FilterGA = changing_run(func = fs.main,
                        paths = ['wbcd.data', 'sonar.data'],
                        feat_names = ['wbcd.names', 'sonar.names'], 
                        n = 5)  
```

```{python }
FilterGA[2][0][0].total_seconds() # THESE RESULTS FOR A SINGLE RUN, CONTINUOUS X 
# gives the time[2] for the dataset [1] and run [0]
# 326 seconds for the big dataset (60 features) over 5 mins
# 97 seconds for the small dataset (30 features) 1.5 mins
WrapperGA[2][0][0].total_seconds() 
# 104 sec (30 feat) nearly 2 min
# 94 seconds for big one 1.5 min
###### breakdown
# GA_obj[0=stats[dat[run[gen]]], 1=hof[dat[run[indiv]]], 2=time[dat[run[delta]]], 3=pop[dat[run[indiv[chromosome]]]]60
```


