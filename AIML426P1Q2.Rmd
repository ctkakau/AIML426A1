---
title: "AIML426 Project 1 Q2"
author:  Chad Kakau
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
library(reticulate)
```

# AIML426 Project 1 Q2  

## Genetic Algorithm for Feature Selection  

Genetic algorithm can be used for feature selection, given $N$ features, each feature selection result can be represented as a $N$-dimensional binary list $X=(x_1, ..., x_n)$, where $x_i=1$ means the feature $i$ is selected and $x_i=0$ otherwise.  

### Problem description  

Take a dataset with $N$ features and determine the optimal selection of features for fitting a good predictive model.  The task is to build a Genetic Algorithm to perform feature selection, by selecting the fewest features that provide good classification.  The task requires two approaches:  

  - __Filter function__:  where the fitness evaluation is conducted without a classifier function  
  - __Wrapper function__:  where the fitness evaluation is conducted by assessing the classification accuracy of the features when used with a classification model.  
  
## overall process

This genetic programme follows a generic process:  

```
__Initialise__ (population
    create Fitness_class
      min_class:  classification_accuracy/(m=count_selected_features)
      min_feat:  (m=count_selected_features)
    create Individual_class
      inherit Fitness_class
    individual representation
      define chromosomes:
        'expr': boolean
      define individual:
        'indiv': 'expr'*n
    initialise population:
      'pop': initialise_'indiv' * pop_size=n
__Repeat__ until __Stop__  
__Evaluate__ individuals  
    classification_accuracy
    m
__Crossover__ 
    parents_selected(prob=0.6)
      cxOnePoint(parent1, parent2)
        bit_selected(prob=0.4)
__Mutate__ 
    individual_selected(prob=0.4)
      mutFlipBit(indiv)
      bit_selected(prob=0.4)
__Selection (Tournament)__ offspring  
__if__ stopping_criteria or max_generations=50:  
  __Stop__   
__else__ go_to __Evaluate__
```

### evaluation function:  FilterGA  

The Filter function compares average mutual gain information per feature.  The evaluation function computes the information gain (using the sklearn.feature_selection.mutual_info_classif function) for the subset of features for an individual (i.e. features where $x_i = 1$) and then averages that information gain across the selected features, to give an average information gain per feature.  The sklearn.feature_selection.mutual_info_classif function allows for identification of discrete variables and can handle continuous variables.  

The mutual_info_classif function is passed continuous predictor variables DISCRETISE X FIRST, THEN PASS THE DISCRETISED TABLE - WILL THAT BE FASTER? 

Average information gain per feature has a maximising objective.
Number of features has a minimising objective. 

$$
\begin{aligned}
\text{Fit}_{avg IG} & = \frac{I(Y;X)}{m}, \text{ with } m = \text{no. of features in X}\\
 \\
 I(Y;X)& = H(Y)-H(Y|X)\\
\text{where } H(Y) & = -\sum_y p(y)*log_2 p(y),\\
 H(Y|X) & = \sum_{x_1, ..., x_m} p(x_1, ...,x_m)H(Y|X_1 = x_1, ..., X_m = x_m)\\
 & = - \sum_{x_1, ..., x_m} \sum_y p(x_1, ..., x_m) *p(y|x_1, ..., x_m)*\log_2 p(y|x_1, ..., x_m)
\end{aligned}
$$
### evaluation function:  Wrapper

The Wrapper function uses 

### M

```{python importAndFuncCreation, include=FALSE}
# import function
import feat_select as fs


# function for running experiment N-times
def multi_run(func, data = None, feat_names = None, params = None, n=1):
    from datetime import datetime as dt
    # accept only singular parameters - i.e. feed one function only
    best_indiv = []
    run_stats = []
    time_delta = []
    population = []

    for r in range(n): 
        start = dt.now()
        if feat_names:
            if params:
                pop, stat, hof = func(data, feat_names, params)
                
            else:
                pop, stat, hof = func(data, feat_names)
                
        else: 
            pop, stat, hof = func(data, params)

        
        end = dt.now()
        time = end - start

        run_stats.append(stat)
        best_indiv.append(hof)
        time_delta.append(time)
        population.append(pop)
                
    return run_stats, best_indiv, time_delta, population

# function for running through the different datasets or different functions
def changing_run(paths = None, feat_names = None, func = None, params = None, n = 1):

    diffs = [[],[], [], []] 

    if len(paths) > 1:
        # iterate paths, keep all other parameters the same, function is required, params are optional
        for p, dat in enumerate(paths):
            if len(feat_names) > 1:
                # include feat_names input
                if params:
                    run, best, time, population = multi_run(func, 
                                           data= dat, 
                                           feat_names= feat_names[p], 
                                           params = params, 
                                           n = n)
                else:
                    run, best, time, population = multi_run(func, 
                                           data= dat, 
                                           feat_names= feat_names[p], 
                                           n = n)
            
            else:
                
                if params:
                    run, best, time, population = multi_run(func, 
                                           data= dat, 
                                           params = params, 
                                           n = n)
                else:
                    run, best, time, population = multi_run(func, 
                                           data= dat, 
                                           n = n)
            diffs[0].append(run)
            diffs[1].append(best)
            diffs[2].append(time)
            diffs[3].append(population)

    return diffs
```

```{python wrapperExperiments, include = FALSE}
# test the wrapper function
WrapperGA = changing_run(func = fs.main,
                         params = "FEAT_SEL = 'Wrapper'",
                         paths = ['wbcd.data', 'sonar.data'],
                         feat_names = ['wbcd.names', 'sonar.names'],
                         n = 5)  
```

```{python filterExperiments, include = FALSE}
# test the filter function
FilterGA = changing_run(func = fs.main,
                        paths = ['wbcd.data', 'sonar.data'],
                        feat_names = ['wbcd.names', 'sonar.names'], 
                        n = 5)  
```

```{python }
FilterGA[2][0][0].total_seconds() # THESE RESULTS FOR A SINGLE RUN, CONTINUOUS X 
# gives the time[2] for the dataset [1] and run [0]
# 326 seconds for the big dataset (60 features) over 5 mins
# 97 seconds for the small dataset (30 features) 1.5 mins
WrapperGA[2][0][0].total_seconds() 
# 104 sec (30 feat) nearly 2 min
# 94 seconds for big one 1.5 min
###### breakdown
# GA_obj[0=stats[dat[run[gen]]], 1=hof[dat[run[indiv]]], 2=time[dat[run[delta]]], 3=pop[dat[run[indiv[chromosome]]]]60
```

```{python readInDataForFunction}
import pandas as pd
df = pd.read_csv('wbcd.data', header = None)
X = df.iloc[:, len(df.columns)-1]
```

```{python testingForFunction}
del X_disc
X_disc = X

for i in range(len(df.columns)-1):
  X_disc[:, i] = (pd.cut(df.iloc[:, i], 10))

X_disc

```

```{python compareDiscToCont}
from datetime import datetime
from sklearn.feature_selection import mutual_info_classif
X_cont = df.iloc[:, :30]
y = df.iloc[:, 30]

s1 = datetime.now()
cont = mutual_info_classif(X_cont, y)
e1 = datetime.now()
t1 = e1-s1

s2 = datetime.now()
disc = mutual_info_classif(X_disc, y)
e2 = datetime.now()
t2 = e2-s2


```

